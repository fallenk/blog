---
title: 剪枝网络
date: 2019-05-20 21:14:25
tags: [paperReading]
category: [论文阅读]
---

The Lottery Ticket Hypothesis: Finding sparse, trainable neural networks

抽奖彩票假说: 寻找稀疏，可训练的神经网络

<!--more-->

📝心得：实验结果扎实可信(改进明显)，主题简单明确，未来科研究方向多

# 摘要

神经网络剪枝技术可以将**训练好的神经网络参数**减少90%以上，在不影响准确率的情况下，降低存储要求并提高计算性能。然而当前的经验的是: 通过剪枝产生的稀疏参数矩阵从头开始训练很困难，同样也很难提高训练性能。

我们发现标准的剪枝技术揭示了一些子网络，这些**子网络的初始化可以进行高效的训练**。基于以上这些结果，我们阐明了彩票假说：**当进行隔离训练时，密集、随机初始化、前向传播的网络包含子网络(获胜的彩票)在同样的迭代次数和在测试集上将会达到与原始网络相同的精度。**我们发现的子网络(the winning tickets) 赢过了初始的抽彩给奖法(the initialization lottery)：他们之间的联系具有初始的权值使训练更加高效。

!作者提出了一个算法来**识别wining tickets**，以及一系列来**支持彩票假说**和**随机初始化**的重要性的实验。我们持续发现子网络(the winning tickets)比全接连和卷积前向网络的10-20%还少，在MNIST和CIFAR10数据集上。超过这个大小，我们发现子网络(the winning tickets)比原始网络更快达到更高的准确率。

# Introduction

从神经网络中消除不必要的权重(剪枝)(LeCun等，1990; Hassibi＆Stork，1993; Han等，2015; Li等，2016)可以减少90%以上的参数数量并不减少准确率。这样做降低了训练的神经网络的规模和能源消耗，使推理更加高效。然而，如果一个神经网络可以被减小规模，为什么我们不训练一个更小的参数结构，反而注意在让训练更高效？当代的经验是说**通过剪枝的结构从一开始难以训练，其准确率会低于原始准确率**。

思考一个例子。在图1，我们**随机采样和训练全连接和卷积网络的子网络**在mnist和cifar10的数据集上。**随机抽样**(Random sampling)模拟了LeCun等人使用的**非结构化剪枝的效果**。（1990）和Han等人。 （2015年）。在各种稀疏度水平上，虚线追踪在迭代中的最小验证集的损失和测试精度。**网络越稀疏，学习越慢，最终的测试精度越低。**

[1]  从头开始训练修剪过的模型比重新训练之前之前修剪过模型更糟糕，这可能表明训练网络容量小的困难。”（Li et al。，2016）“在再训练期间，**最好保留权重修剪过程中存在的连接的初始训练阶段**比重新初始化修剪过的图层...**梯度下降能够在网络初始训练时找到一个好的解决方案，但不能在重新初始化某些层并重新训练它们之后找到一个好的解决方案**“。

[2]作为网络学习速度的代理，我们使用迭代，在该迭代中，早期停止的标准是将结束训练。我们在本文中使用的迭代是早期停止标准是**训练期间最小验证损失的那个迭代**。有关更多详细信息，请参阅附录C. 注:  在多次迭代中，我们选取那个迭代== 一个早期停止的标准用来结束训练。早期停止标准是**训练期间最小验证损失的那个迭代**

![](./剪枝网络/1.png)

图1：MNIST的Lenet架构和CIFAR10的Conv-2，Conv-4和Conv-6架构的早期停止（左）和迭代（右）的迭代次数（见图2）从不同尺寸开始训练时。虚线是**随机抽样的稀疏网络**（十次试验的平均值）。实线是赢彩票the winning tickets（五次试验的平均值）密集、随机初始化、前向传播。

**左1：X轴剪枝后保留的权重的比例，Y轴早期停止的迭代 即 在多轮迭代中选取的最小验证集的损失的那一次迭代。 wining tickets 在相同的比例的参数下，收敛速度更快**

**左2：X轴剪枝后保留的权重的比例，Y轴早期停止的迭代 即 在多轮迭代中选取的最小验证集的损失的那一次迭代。不同网络保留的网络参数越多，需要达到最小验证集损失的训练迭代次数也越大。相同网络参数越少，达到训练标准的次数越大。**

**右1：X轴剪枝后保留的权重的比例，Y轴：达到最小验证集损失时在测试集上的准确率。保留的参数比例由大到小时，准确率先升后降，wining tickets效果好于随机采样。**

**右2：X轴剪枝后保留的权重的比例，Y轴：达到最小验证集损失时在测试集上的准确率。保留的参数比例由大到小时，准确率先升后降，复杂网络准确率越高。**

在本文中，我们表明，**一直存在较小的子网络**，从一开始就训练，并且至少与其较大的对应网络一样快速地学习，同时达到相似的测试精度。**图1中的实线显示了我们发现的网络**。基于这些结果，我们陈述了彩票假设。

**The Lottery Ticket Hypothesis.**彩票假设。**随机初始化**的、**密集的**、神经网络包含一个子网络，经过初始化，当经过隔离训练时，它可以达到训练后最多相同迭代次数的原始网络的测试精度。

更正式的，考虑一个密集的、前向传播的网络$f(x ; \theta)$,初始化参数为$\theta=\theta_{0} \sim \mathcal{D}_{\theta}$。当在训练集上用随机梯度下降SGD优化时，$f$经过$j$轮迭代达到了最小的验证损失$l$，测试正确率为$a$。 另外，考虑用掩码$m \in\{0,1\}^{|\theta|}$训练$f(x ; m \odot \theta)$，参数初始化为$m \odot \theta_{0}$。当在同样的训练集(m固定)上用SGD来优化时，$f$达到了最小的验证损失$l^{\prime}$

经过了$j^{\prime}$轮迭代，达到了测试正确率$a^{\prime}$. 

**彩票假说预测 $\exists m$使 $j^{\prime} \leq j$ (相应的训练时间)，$a^{\prime} \geq a$(相应的正确率)和$\|m\|_{0} \ll|\theta|$ (更少的参数)。**

我们发现**标准的剪枝技术会自动从全连接和卷积的前馈网络中发现这种可训练的子网络**。我们**指定这些可训练的子网络$f(x ; m \odot \theta)$ 是中奖票(winning tickets)**，因为我们发现的那些已经通过能够学习的权重和连接的组合超过了(the initialization lottery)初始化抽奖。当他们的参数**随机重新初始化**($f\left(x ; m \odot \theta_{0}^{\prime}\right) \text { where } \theta_{0}^{\prime} \sim \mathcal{D}_{\theta}$)，我们的中奖票(winning tickets)不再匹配原始网络的性能，提供证据表明这些较小的网络不能有效训练，除非他们适当初始化。

**Identifying winning tickets.** 

我们通过**训练网络并剪枝其最小等级的权重来找到获胜彩票**。剩余的未剪枝通过连接构成了中奖票的参数结构。对于我们的工作独一无二而言，**每个未经传输的连接的权值在被训练之前将被重置为从原始网络初始化**。这形成了我们的中心实验：

1. 随机初始化神经网络$f\left(x ; \theta_{0}\right)\left(\text { where } \theta_{0} \sim \mathcal{D}_{\theta}\right)$
2. 训练一个网络 用 $j$轮迭代，达到参数 $\theta_{j}$ [ $f$经过$j$轮迭代达到了最小的验证损失$l$，测试正确率为$a$。]
3. 剪枝$p\%$的$\theta_{j}$参数，通过创建一个mask $m$  [剪枝 最小等级的权值]
4. 把$\theta_{0}$中的未剪枝的参数值重新设置到保留的参数中，得到the winning tickets $f\left(x ; m \odot \theta_{0}\right)$

如上所述，这种修剪方法是一次性的：一次网络训练，p％的权重被修剪，幸存的权重被重置。然而，在本文中，我们专注于**迭代修剪，反复训练，修剪和重置网络**n轮 rounds; 每一rounds剪枝掉上一轮保留参数的$p^{\frac{1}{n}} \%$ 。我们结果证明 迭代剪枝发现中彩票 比得上原始网络的正确率，参数规模小于一次性剪枝的规模。

**Results.**我们辨识找到winning tickets 在全连接结构对mnist和卷积结构对cifar10 通过几个优化策略(SGD, 动量, adam) 用不同技术如删除，权重衰退，batchnorm和残差连接。我们用无结构化的剪枝技术，所以中票的彩票是稀疏的。在更深的网络中，**我们这个基于剪枝的策略 来找到 winning tickets 对学习率是很敏感的**：**它需要热身warmup才能在以更高的学习率获得中奖彩票**。我们发现的中奖彩票(winning tickets)是原始网络规模的**10-20％**（或更小）（较小的尺寸）。在这个尺寸下，它们在最多相同的迭代次数（相应的训练时间）内达到或超过原始网络的测试精度（相称的准确度）。当随机重新初始化时，中奖彩票(the winning tickets)表现得更糟，这意味着单独的结构无法解释获胜彩票成功。

**The Lottery Ticket Conjecture.**彩票票猜想。回到我们的动机问题，我们将我们的假设扩展为一个未经测试的猜想，即**SGD寻找并训练一组良好初始化的权重**。密集，随机初始化的网络比剪枝产生的稀疏网络更容易训练，因为有更多可能的子网络，训练可以从中恢复中奖票参数。

**Contributions.**

- 我们证明**剪枝可以找到可训练的子网络，这些子网络达到了测试精度**，与原始网络相比，它们可以在相同数量的迭代中得到。
- 我们表明**剪枝可以获得比原始网络学得更快的中奖彩票参数**，同时达到更高的测试精度和更好的推理。
- 我们提出将**彩票假设作为解释这些发现的神经网络**组成的新视角。

![](./剪枝网络/2.png)

图2：本文测试的体系结构。卷积是3x3。 Lenet来自LeCun等人。 （1998）。 Conv-2/4/6是VGG的变种（Simonyan＆Zisserman，2014）。 Resnet-18来自He等人。 （2016）。 CIFAR10的VGG-19改编自Liu等人。 （2019）。初始化是Gaussian Glorot（Glorot＆Bengio，2010）。括号表示层周围的残余连接。

**Implications.**在本文中，我们实证研究了彩票假设。既然我们已经证明了中奖票的存在，我们希望利用这些知识：

- **提高训练性能**。**由于中奖彩票(winning tickets)可以从一开始就被隔离训练，我们希望我们可以设计出尽可能早地搜索中奖彩票和剪枝的训练方案**。 
- **设计更好的网络**。获奖票(Winning tickets)证揭示了稀疏的**架构和初始化的组合**，这些组合特别适应学习。我们可以从中奖彩票(winning tickets)中获取灵感，设计具有有助于学习的**相同属性的新架构和初始化方案**。我们甚至可以将为一项任务发现的中奖票转移给许多其他人。
- **提高我们对神经网络的理论认识**。我们可以研究为什么随机初始化的前馈网络似乎包含中奖票和对优化理论研究的潜在影响（Du等，2019）和泛化（Zhou等，2018; Arora等，2018）。

# 2 Winning tickets在全连接网络中

在本节中，我们评估了应用于在MNIST上训练的全连接网络的彩票假设。我们使用Lenet-300-100架构（LeCun等，1998），如图2所示。我们遵循第1节中的大纲：在随机初始化和训练网络之后，我们修剪网络并将剩余连接重置为其原始初始化的参数值。我们使用**简单的逐层修剪**启发：**删除每层中具有最低幅度的权重的百分比**（如Han等人（2015））。**与输出的连接的部分以网络其余部分比率的一半进行修剪**。我们将在附录G中探索其他超参数，包括学习速率，优化策略（SGD，动量），初始化方案和网络规模。

![](./剪枝网络/3.png)

图3：随着训练的进行，Lenet（迭代修剪）的测试准确性。每条曲线是五次试验的平均值。标签是$P_{m}$  - 修剪后网络中剩余的权重比重。误差线是任何试验的最小值和最大值。

**Notation**符号. $P_{m}=\frac{\|m\|_{0}}{|\theta|}$ 是mask m的稀疏程度，比如： $P_{m}=25 \%$是表示75%的参数被剪掉

G.5 I NITIALIZATION D ISTRIBUTION

To this point, we have considered only a Gaussian Glorot (Glorot & Bengio, 2010) initialization scheme for the network. Figure 30 performs the lottery ticket experiment while initializing the Lenet architecture from Gaussian distributions with a variety of standard deviations. The networks were optimized with Adam at the learning rate chosen earlier. The lottery ticket pattern continues to appear across all standard deviations. When initialized from a Gaussian distribution with standard deviation 0.1, the Lenet architecture maintained high validation accuracy and low early-stopping times for the longest, approximately matching the performance of the Glorot-initialized network.

到目前为止，我们只考虑了网络的Gaussian Glorot（Glorot＆Bengio，2010）初始化方案。图30执行彩票实验，同时从具有各种标准偏差的高斯分布初始化Lenet架构。亚当以之前选择的学习率对网络进行了优化。彩票模式继续出现在所有标准偏差中。当从具有标准偏差0.1的高斯分布初始化时，Lenet架构保持高验证准确度和较低的早停时间，最长，与Glorot初始化网络的性能大致匹配。

## **Iterative pruning.** 

迭代剪枝。 我们获得的中奖彩票（The winning tickets）比原来的网络学得更快。图3绘制了在不同程度上迭代修剪的中奖彩票（The winning tickets）时的平均测试精度。误差线是五次运行的最小值和最大值。对于第一次修剪，网络学习得越快，测试精度越高，修剪得越多（图3中的左图）。中奖彩票，占原网络重量的51.3％(.i.e,  $P_{m}=51.3 \%$)比原始网络更快地达到更高的测试准确度，但比P m = 21.1％时更慢。当P m <21.1％时，学习速度减慢（中图）。当P m = 3.6％时，中奖票据会回归到原始网络的性能。本文中重复了类似的模式。

在每次迭代中的**迭代修剪**20％时（蓝色）, 图4 a 总结了所有修剪级别方法的此行为。左边是每个网络达到最小验证损失的迭代（即，当早期停止标准将停止训练时）与修剪后剩余的权重百分比相关的迭代;中间是该迭代的测试精度。我们使用迭代来满足早期停止标准，作为网络学习速度的代理。

当P m从100％降低到21％时，The winning tickets学得更快，此时提前停止比原始网络早38％ 原始-5K，prune- 3.1k。进一步修剪导致学习速度变慢，当P m = 3.6％时，返回到原始网络的早期停止性能。随着修剪，测试集精度提高，当P m = 13.5％时，测试精度提高0.3个百分点以上;在此之后，当P m = 3.6％时，精度降低，返回到原始网络的水平。

在早期停止时，训练准确性（图4a，右）随着修剪以类似的模式增加以测试准确性，似乎暗示winning tickets更有效地优化但不能更好地推广。然而，在迭代50,000（图4b），尽管几乎所有网络的训练精度达到100％，但迭代修剪的winning tickets仍然可以看到测试精度提高高达0.35个百分点（附录D，图12）。这意味着，winning tickets的训练准确度和测试准确度之间的差距较小，这表明泛化的改进。

## **Random reinitialization.**

随机重新初始化。为了测量a winning ticket的**初始化的重要性**，我们保留a winning ticket的结构（即，掩码m），但是随机地采样新的初始化参数$\theta_{0}^{\prime} \sim \mathcal{D}_{\theta}$。我们随机重新初始化(each winning ticket)每张中奖彩票三次，在图4中每点累计15次。我们发现初始化对获胜彩票的效果至关重要。图3中的右图显示了迭代修剪的这个实验。除了原始网络和P m = 51％和21％的获奖门票是随机重新初始化实验。**the winning tickets获奖彩票在被修剪时学得更快的地方，但随机重新初始化时，它们学得越来越慢。**

该实验的更广泛的结果是图4a中的橙色线。**与获奖彩票不同，重新初始化的网络学习速度比原始网络慢，并且在修剪后会失去测试准确性**。平均重新初始化的迭代中奖票的测试精度从原始准确度下降，相比于当the winning ticket的P m = 21.1％时和2.9％。当P m = 21％时，中彩票的最小验证损失比重新初始化时快2.51倍，并且准确度提高了半个百分点。所有网络达到100％的训练精度，Pm≥5％;  因此, 图4b表明**中彩票比随机重新初始化时更好地泛化。该实验支持彩票假设'强调初始化：原始的初始化承受和修剪的好处，而随机重新初始化的性能立即受到影响并且稳定地减少。**

![](./剪枝网络/4.png)

图4：在一次性和迭代修剪下Lenet的早期停止迭代和准确性。平均五次试验;最小值和最大值的误差线。在迭代50,000，对于迭代中奖票，Pm≥2％的训练精度≈100％（见附录D，图12）。

## One-shot pruning.

一次性剪枝。**虽然迭代修剪iterative pruning提取较小的中奖票winning tickets，但重复地训练意味着它们被找到的代价大。**一次性修剪可以在没有重复训练的情况下识别获奖彩票winning tickets。图4c显示了一次性修剪（绿色）和随机重新初始化（红色）的结果; **一次性修剪确实能获得中奖彩票**。当67.5％≥Pm≥17.6％时，平均中奖彩票比原始网络更早达到最低验证准确度。当95.0％≥Pm≥5.17％时，测试精度高于原始网络。但是，迭代修剪的获奖彩票可以更快地学习，并在较小的网络规模下达到更高的测试精度。图4c中的绿线和红线在图4a的对数轴上再现，使得该性能差距清晰。**由于我们的目标是确定最小的获奖门票，因此我们将重点放在本文其余部分的迭代修剪上。**

# 3 Winning tickets在卷积网络中

在这里，我们将彩票假设应用于CIFAR10上的卷积网络，增加了学习问题的复杂性和网络的规模。我们考虑图2中的Conv-2，Conv-4和Conv-6架构，它们是VGG（Simonyan＆Zisserman，2014）系列的缩小版本。网络有两个，四个或六个卷积层，后面是两个完全连接的层;每两个卷积层之后发生最大池化。这些网络覆盖了从近乎完全连接到传统卷积网络的范围，其中参数少于Conv-2中卷积层中的参数不到的1％和Conv-6中的接近三分之二。

## Finding winning tickets.

图5（上）中的实线显示了Conv-2（蓝色），Conv-4（橙色）和Conv-6（绿色）的迭代彩票实验来自图2中的每层修剪率。来自Lenet的第2节重复：随着网络被修剪，与原始网络相比，它学得更快，测试准确度也提高了。在这种情况下，结果更加明显。

![](./剪枝网络/5.png)

图5：在迭代修剪和随机重新初始化时，Conv-2/4/6架构的早期停止迭代和测试及训练准确性。**每条实线是五次试验的平均值**; **每条虚线是15次重新初始化的平均值（每次试验三次）**。右下图描绘了在与原始网络的最后一次训练迭代相对应的迭代中获胜门票的测试准确度（Conv-2为20,000，Conv-4为25,000，Conv-6为30,000）;在此迭代中，对于中奖彩票，Pm≥2％的训练精度≈100％（见附录D）。 达到50K，traning accuracy = 100%



中奖彩票达到最低验证损失比为Conv-2的快3.5倍（P m = 8.8％），比Conv-4快3.5倍（Pm = 9.2％），比Conv-6快2.5倍（P m = 15.1％） 。测试精度最高提高3.4个百分点对Conv-2的（P m = 4.6％），Conv-4的3.5个百分点（P m = 11.1％）和Conv-6的3.3个百分点（P m = 26.4％）。当P m> 2％时，所有三个网络都保持在其原始平均测试精度之上。 ???

与第2节一样，早期停止迭代的训练精度随着测试精度而提高。然而，在Conv-2的迭代20,000，Conv-4的25,000和Conv-6的30,000（对应于原始网络的最终训练迭代的迭代）中，**当Pm≥2%时，所有网络的训练精度达到100％（附录D，图13）和中彩票仍然保持较高的测试精度（图5右下）。这意味着中彩票的测试和训练准确度之间的差距较小，表明它们泛化更好。**

## Random reinitialization.

随机重新初始化，我们重复随机重新初始化实验按照第2节，即在图5中展示的虚线。这些网络再次花费更长的时间来学习在继续修剪时。正如MNIST上的Lenet（第2节）一样，随机重新初始化实验的测试精度下降得更快。然而，与Lenet不同，**早期停止时的测试准确性最初保持稳定，甚至在Conv-2和Conv-4上也得到改善，表明 - 在适度的修剪 - 中奖彩票的结构可能会带来更好的准确性**。

## Dropout

Dropout（Srivastava et al。，2014; Hinton et al。，2012）通过在每次训练迭代中**随机排除一小部分单元（即，随机抽样子网）**来提高准确性。Baldi＆Sadowski（2013）将Dropout描述为同时训练所有子网的整体。由于彩票假设**表明这些子网中的一个包含中奖彩票**，因此很自然地会询问Dropout和我们发现中奖票的策略是否相互影响。

图6显示了Conv-2，Conv-4和Conv-6的训练结果，其dropout率为0.5。虚线是没有dropout的网络性能（图5中的实线）。 我们在dropout训练时继续获得中奖彩票。**dropout提高了初始测试的准确性（分别为Conv-2，Conv-4和Conv-6的平均值分别为2.1,3.0和2.4个百分点），迭代修剪进一步提高（最多增加2.3,4.6和4.7百分点，平均分别）。迭代修剪学习变得比以前更快，但在Conv-2的情况下则不那么显着。**

![](./剪枝网络/6.png)

图6：Conv-2/4/6早期停止时的早期停止迭代和测试精度，当经过反复修剪和dropout训练时。虚线是在没有dropout的情况下训练的相同网络（图5中的实线）。 Conv-2的学习率为0.0003，Conv-4和Conv-6的学习率为0.0002。

![](./剪枝网络/7.png)

图7：迭代修剪时VGG-19的测试精度（在30K，60K和112K迭代）。

**这些改进表明我们的迭代修剪策略与dropout相互促进在互补的作用下。 Srivastava观察到dropout导致最终网络中的稀疏激活;  dropout引起的稀疏性可能会使网络被修剪。如果是这样，针对权重的dropout技术（Wan et al。，2013）或学习每个权重的dropout概率（Molchanov等，2017; Louizos等）可以使中奖彩票更容易找到。**

# 4 VGG 和Resnet对CIFAR10的数据

在这里，我们研究了神经网络上的彩票假设，这些假设唤起了实践中使用的架构和技术。具体而言，我们考虑VGG型深度卷积网络（CIFAR10-Simonyan和Zisserman（2014）上的VGG-19）和残余网络（CIFAR10-He等人（2016）上的Resnet-18）。这些网络**通过batchnorm，权重衰减，降低学习率计划和增强的训练数据**进行训练。我们继续为所有这些架构找到中奖彩票; 然而，我们发现它们的方法，**迭代修剪，对所使用的特定学习率很敏感**。在这些实验中，我们不是测量早期停止时间（对于这些较大的网络，与学习速率计划纠缠在一起），**而是在训练期间的几个时刻绘制准确度，以说明准确度提高的相对速率**。

## Global pruning.

全局剪枝。在Lenet和Conv-2/4/6上，我们以相同的比例分别修剪每一层。对于Resnet-18和VGG-19，我们稍微修改了这个策略：**我们在全局范围内修剪这些更深层的网络，在所有卷积层中共同去除最低幅度的权重**。在附录I.1中，我们发现全局修剪确定了Resnet-18和VGG-19的较小中奖票。我们对此行为的推测解释如下：对于这些更深层次的网络，某些层具有比其他层更多的参数。例如，VGG-19的前两个卷积层有1728和36864个参数，而最后一个有235万个。**当所有图层以相同的速率修剪时，这些较小的图层成为瓶颈，使我们无法识别最小的中奖票。全局修剪可以避免这种陷阱。**

## VGG-19.

我们研究了Liu等人的适用于CIFAR10的变体VGG-19。 （2019）;我们使用相同的训练方案和超参数：160个epochs（112,480次迭代），其中SGD具有动量（0.9）并且在80和120个时期将学习率降低10倍。该网络有2000万个参数。图7显示了在**两个初始学习率下对VGG-19进行迭代修剪和随机重新初始化的结果**：0.1（在Liu等人（2019）中使用）和0.01。**在较高的学习速率下，迭代修剪不能找到中奖票，并且性能并不比修剪后的网络随机重新初始化时更好**。**但是，在较低的学习速率下，通常的模式重新出现，子网络保持在原始精度的1个百分点内，**而Pm≥3.5％。 （**他们不是中彩票，因为它们与原始准确度不匹配。**）当随机重新初始化时，子网络会失去准确性，因为它们与本文中的其他实验一样被修剪。尽管这些子网在训练早期学习的速度比未传播的网络快（图7左），但**由于较低的初始学习速率，这种准确性优势在训练后期会逐渐消失。但是，这些子网仍然比重新初始化时学得更快。**

![](./剪枝网络/8.png)

图8：迭代修剪时Resnet-18的测试精度（在10K，20K和30K迭代）。

为了**弥合较低学习率的彩票行为与较高学习率的准确性优势之间的差距**，我们探讨了从0到初始学习率的线性学习率预热(warmup)对k次迭代的影响。在学习率0.1下以预热(warmup)（k = 10000，绿线）训练VGG-19可以将未剪枝网络的测试精度提高大约一个百分点。热身(warmup)可以获得中奖彩票，**当Pm≥1.5％时超过此初始准确度**。

## Resnet-18.

Resnet-18（He et al。，2016）是一个20层卷积网络，具有为CIFAR10设计的残差连接。它有271,000个参数。我们用SGD以动量（0.9）训练网络进行30,000次迭代，在20,000和25,000次迭代时将学习率降低10倍(decreasing the learning rate by a factor of 10)。图8显示了学习率0.1（在He等人（2016）中使用）和0.01的迭代修剪和随机重新初始化的结果。这些结果很大程度上反映了VGG的结果：**迭代修剪以较低的学习率获得中奖彩票，但不是较高的学习率**。较低学习率的最佳中奖门票的准确率（当41.7％≥Pm≥21.9％时，89.5％）在较高学习率（90.5％）下达不到原始网络的准确读。**在较低的学习率下，中奖票最初学习得更快（图8的左图），但是在训练后的较高学习率（右图）下落后于未剪枝的网络。**通过预热训练的中奖彩票在较高学习率下与未剪枝网络的准确性差距接近，在P m = 27.1％时，学习率0.03（预热，k = 20000）达到90.5％的测试准确度。对于这些超参数，当Pm≥11.8％时，我们仍然会获得中奖彩票。然而，**即使有热身warmup，我们也无法找到超参数在我们可以用原始学习率0.1来识别中奖彩票。**

# 5 讨论！

关于神经网络修剪的现有工作（例如，Han等人（2015））证明**由神经网络学习的函数通常可以用较少的参数来表示**。修剪通常通过**训练原始网络**，**移除连接**和**进一步调整**来进行。实际上，**刚开始的训练初始化修剪网络的权重**，以便它可以在微调期间孤立地学习。我们试图确定**类似的稀疏网络是否可以从一开始就学习**。我们发现本文研究的**架构可靠地包含这种可训练的子网**，并且彩票假设提出该属性一般适用。**我们对获奖门票的存在和性质的实证研究引发了许多后续问题。**

## The importance of winning ticket initialization.

**winning tickets 初始化的重要性**。当随机重新初始化时，winning tickets学得更慢并且测试精度更低，这表明初始化对其成功很重要。这种行为的一种可能的解释是**这些初始权重在训练之后接近它们的最终值** - 在最极端的情况下，它们已经被训练。然而，附录F中的实验表明相反 - **中奖票(winning tickets)权重比其他权重更进一步**。这表明初始化的优势与**优化算法**，**数据集**和**模型**相关联。例如，the winning ticket的初始化可能落在损失情况的区域中，该区域特别适合于通过所选择的优化算法进行优化。

刘等人发现当被随机重新初始化时，被修剪的网络确实可训练，似乎与传统经验和我们的随机重新初始化实验相矛盾。举个例子，在VGG-19（我们共享相同的设置）上，他们发现**网络被修剪高达80％并且随机重新初始化匹配原始网络的准确性**。我们在图7中的实验证实了这种稀疏程度的这些发现（Liu等人没有提供数据）。然而，**在进一步修剪之后，初始化很重要**：当VGG-19被修剪高达98.5％时我们获得了winning tickets; 重新初始化时，这些winning tickets的准确度要低得多。我们假设 - 达到一定程度的稀疏性 - 高度参数化的网络可以被成功修剪，重新初始化和重新训练; 然而，超越这一点，极度修剪，不太严重的过参化的网络只能通过偶然的初始化来保持准确性。

## The importance of winning ticket structure.

**winning tickets的参数结构**。产生中奖彩票的初始化安排在特定的稀疏架构中。由于我们通过大量使用训练数据来发现中奖彩票，我们假设我们的中奖彩票的**结构编码为手边的学习任务定制的归纳偏差**。Cohen＆Shashua（2016）表明，**嵌入在深层网络结构中的归纳偏差决定了它可以比浅层网络更有效地分离更多参数的数据类型; ** 虽然Cohen＆Shashua（2016）专注于卷积网络的汇集几何，但类似的效果可能与获奖门票的结构有关，即使在大量修剪时也可以学习。

## ？The improved generalization of winning tickets.

winning tickets的改进泛化。**我们可靠地找到能够更好地泛化的中奖彩票，超过原始网络的测试精度，同时匹配其训练准确性。** **测试精度随着我们的修剪而增加然后减小**，形成一个Occam's Hill（Rasmussen＆Ghahramani，2001），其中原始的，过度参数化的模型具有太多的复杂性（可能过度配置）并且极度修剪的模型太少。压缩和泛化之间关系的传统观点是紧凑的假设可以更好地泛化（Rissanen，1986）。最近的理论工作显示了神经网络的类似链接，证明了可以进一步**压缩的网络的更紧密的泛化界限**（Zhou等人（2018）用于修剪/量化，而Arora等人（2018）用于噪声鲁棒性）。**彩票假设提供了对这种关系的补充观点 - 较大的网络可能明确地包含更简单的表示。**

## Implications for neural network optimization.

神经网络优化的意义。Winning tickets的准确度可以达到原始未剪枝网络的准确度，但参数显着减少。这一观察结果与**最近关于过度参数化在神经网络训练中的作用有关**。例如，Du等人。 （2019）证明用SGD训练的足够过度参数化的双层relu网络（具有固定大小的第二层）会聚到全局最优。因此，一个关键问题是，winning ticket的存在是必须的？或者满足SGD优化，以便将神经网络优化到特定的测试精度。我们推测（但不是凭经验表明）**SGD寻找并训练一个初始化良好的子网**。通过这种逻辑，**过度参数化的网络更容易训练，因为它们具有更多潜在中奖彩票的子网络组合**。

# 6 缺点与未来工作

我们只在较小的数据集（MNIST，CIFAR10）上考虑以视觉为中心的分类任务。我们不研究更大的数据集（即Imagenet（Russakovsky等，2015））：**迭代修剪是计算密集型的，需要训练网络连续15次或更多次才能进行多次试验**。在未来的工作中，我们打算探索更有效的方法来找到winning tickets，这样就可以在更加资源密集的环境中研究彩票假设。

**稀疏修剪是我们找到中奖彩票的唯一方法**。虽然我们减少了参数计数， 但最终的体系结构并未针对现代库或硬件进行优化。在未来的工作中，我们打算从广泛的当代文献中研究其他修剪方法，例如**结构修剪（这将产生针对当代硬件优化的网络）和非大小修剪方法（可以产生较小的中奖票或更早地找到它们）**。 [非结构化稀疏指的是被稀疏掉（置为0）的权重是随机分布在卷积参数矩阵中的，我们没法控制这些被稀疏掉的权重的位置。GPU的CUDA/CUDNN没法加速，具体数据可以看这里介绍的论文：https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/ 但是如果是自己设计的专用硬件，是可以利用这种稀疏加速的，只是需要专门的硬件支持。]

我们获得的winning tickets具有初始化，这些初始化允许它们匹配未剪枝网络的性能，其大小对于随机初始化的网络太小以至于不能达到同样的效果。在未来的工作中，我们打算研究这些初始化的属性，这些属性与修剪后的网络架构的归纳偏差一致，使这些网络特别擅长学习。

在更深的网络（Resnet-18和VGG-19）上，迭代修剪无法找到winning tickets，除非我们用学习率预热(with learning rate warmup.)训练网络。在未来的工作中，我们计划探索为什么需要预热以及我们的识别winning tickets方案的其他改进是否可以消除对这些超参数修改的需求。

## 7 releated work

在实践中，神经网络倾向于显着过度参数化。蒸馏（Ba＆Caruana，2014; Hinton等，2015）和修剪（LeCun等，1990; Han等，2015）依赖于可以在保持准确性的同时减少参数的事实。即使有足够的能力来记忆训练数据，网络自然也会学习更简单的功能（Zhang et al。，2016; Neyshabur et al。，2014; Arpit et al。，2017）。当代经验（Bengio et al。，2006; Hinton et al。，2015; Zhang et al。，2016）和图1表明，过度参数化的网络更容易训练。我们表明，密集网络包含稀疏子网，能够从原始初始化开始自行学习。其他几个研究方向旨在训练小型或稀疏网络。



# 附录A ACKNOWLEDGMENTS

我们非常感谢IBM，它通过MIT-IBM Watson AI Lab为本文中的实验提供了必要的计算资源。



# 附录B ITERATIVE PRUNING STRATEGIES

在本附录中，我们研究了**构建迭代修剪策略的两种不同方法**，我们在本文的主体中使用该方法来获得中奖票。

**Strategy 1：Iterative pruning with resetting.** 

![image-20190531201016228](./剪枝网络/11.png)

**Strategy 2: Iterative pruning with continued training.** 

![image-20190531201502183](./剪枝网络/12.png)

这两种策略之间的区别在于，在每轮修剪之后，策略2使用已经训练的权重进行重新训练，而策略1在重新训练之前将网络权重重置回其初始值。在这两种情况下，在网络被充分修剪后，其权重将重置为原始初始化。在小尺寸，1比2更优化

图9和图10比较了我们在附录G和H中选择的超参数上Lenet和Conv-2/4/6架构的两种策略。在所有情况下，策略1保持更高的验证准确性和更快的早期停止时间网络规模。



# 附录C early stopping criterion

在整篇文章中，我们感兴趣测量网络学习的速度。作为整个数量的代理，我们测量迭代 是 早期停止的标准将结束训练。我们使用的这个特别的标准是达到最小验证集损失的迭代次数。接下来，我们将解释这个标准。

验证和测试损失遵循一种模式，即**在训练过程的早期阶段减少，达到最小值，然后随着模型过拟合对于训练数据而开始增加**。图11显示了随着训练的进行，验证损失的一个例子;这些图使用Lenet，迭代修剪和Adam，学习率为0.0012。该图显示了与图3中的测试精度相对应的验证损失。

![image-20190531202441709](./剪枝网络/13.png)

图9：使用重置和持续训练策略迭代修剪时，Lenet架构上迭代彩票实验的早期停止迭代和早期停止的准确性。

![image-20190531202619726](./剪枝网络/14.png)

图10：使用重置和持续训练策略进行迭代修剪时，Conv-2，Conv-4和Conv-6架构上迭代彩票实验的早期停止迭代和早期停止的准确性。

![image-20190530212130331](/Users/liulifeng/Workspaces/hexo/source/_posts/剪枝网络/9.png)

图11：对应于图3的验证损失数据，即，在迭代修剪实验中针对若干不同修剪水平的训练进展的验证损失。每条线是在相同的迭代修剪水平下进行的五次训练的平均值;标签是修剪后保留的原始网络权重的百分比。每个网络都使用Adam训练，学习率为0.0012。左图**显示的wining tickets比原始网络学得越快，损失也越来越低**。中间的图表**显示wining tickets，在达到最快的早停时间后学得越来越慢**。右边的图表将**wining tickets的loss与随机重新初始化网络的loss进行了对比**。

Figure 11: The validation loss data corresponding to Figure 3, i.e., the validation loss as training progresses for several different levels of pruning in the iterative pruning experiment. Each line is the average of ﬁve training runs at the same level of iterative pruning; the labels are the percentage of weights from the original network that remain after pruning. Each network was trained with Adam at a learning rate of 0.0012. The left graph shows **winning tickets that learn increasingly faster than the original network and reach lower loss**. The middle graph shows **winning tickets that learn increasingly slower after the fastest early-stopping time has been reached**. The right graph contrasts **the loss of winning tickets to the loss of randomly reinitialized networks**.



![image-20190531200401542](./剪枝网络/10.png)

图12：图4增加了50,000次迭代结束时训练精度的图表。

在所有情况下，验证损失最初都会下降，然后形成一个明确的底部，然后再次开始增加。我们早期停止的标准确定了这个底线。我们认为，更快到达这一时刻的网络可以“更快地”学习。为了支持这一概念，每个实验符合图3中早期停止标准的顺序与每个实验达到**特定测试精度阈值的顺序相同**在图3中。

在本文中，为了使这种学习速度具有背景，我们还在最小验证损失的迭代中呈现网络的测试精度。在论文的主体中，我们发现获奖门票在此时提前到达并提高了测试精度。

# 附录D TRAINING ACCURACY FOR LOTTERY TICKET EXPERIMENTS

本附录附图4（第2节中关于MNIST的Lenet的准确性和早期停止迭代）和图5（第3节中Conv-2，Conv-4和Conv-6的准确性和早期停止迭代）在论文的主体中。这些图表显示了早期停止的迭代，早期停止的测试准确性，早期停止时的训练准确性以及训练过程结束时的测试准确性。但是，我们没有足够的空间在培训过程结束时包含培训准确性的图表，我们在本文的主体中断言，对于除了最严重修剪的网络之外的所有网络都是100％。在本附录中，我们在图12（对应于图4）和图13（对应于图5）中包括那些附加的图。正如我们在论文的主体中描述的那样，**除了最经过重点修剪的网络之外，所有情况下的训练准确率都达到了100％**。但是，获奖门票的培训准确率仍然比随机重新初始化的网络长100％。

# 附录E  COMPARING RANDOM REINITIALIZATION AND RANDOM SPARSITY

比较随机重新定位和随机稀疏性

在本附录中，我们的目标是了解随机重新初始化的中奖票和随机稀疏网络的相对性能。

1. 通过使用原始初始化的迭代修剪找到网络（图14中的蓝色）。
2. 通过迭代修剪找到的网络随机重新初始化（图14中的橙色）。
3. 随机稀疏子网，其参数数量与通过迭代修剪找到的参数数量相同（图14中的绿色）。

![image-20190531204116115](/Users/liulifeng/Workspaces/hexo/source/_posts/剪枝网络/15.png)

图13：图5增加了训练过程结束时训练准确性的图表。

图14显示了本文中所有主要实验的比较。对于MNIST的完全连接的Lenet架构，我们**发现随机重新初始化的网络优于随机稀疏性**。然而，对于本文研究的所有其他卷积网络，两者之间的性能没有显着差异。我们假设MNIST的完全连接网络看到了这些好处，因为只有MNIST图像的某些部分包含有用的分类信息，这意味着网络某些部分的连接将比其他部分更有价值。对于卷积不太正确，卷积不限于输入图像的任何一部分。

# 附录F  EXAMINING WINNING TICKETS

测试wining tickets

在本附录中，我们将检查获奖门票的结构，**以深入了解获奖门票为何能够有效学习，即使经过如此严格的修剪**。在本附录中，我们将研究在MNIST培训的Lenet架构的获奖门票。除非另有说明，否则我们使用与第2节中相同的超参数：**glorot初始化和adam优化**。

## F.1 WINNING TICKET INITIALIZATION (ADAM )

图15显示了四个不同级别P m的中奖票证初始化的分布。为了澄清，这些是在修剪过程中幸存下来的连接的初始权重的分布。蓝色，橙色和绿色线分别显示第一个隐藏层，第二个隐藏层和输出层的权重分布。权重是从彩票实验的五个不同试验中收集的，但每个试验的分布与所有试验中的分布密切相关。直方图已经标准化，因此每条曲线下的面积为1。



# 附录G HYPERPARAMETER EXPLORATION FOR FULLY-CONNECTED NETWORKS

本附录附有主要文件的第2部分。它探讨了第2节中评估的Lenet架构的超参数空间，并考虑了两个目的：

1. 解释在论文正文中选择的超参数。
2. 评估彩票实验模式扩展到其他超参数选择的程度。

## G.1 实验方法

本节考虑了完全连接的Lenet架构（LeCun等，1998），它在MNIST数据集上包含两个完全连接的隐藏层和一个十单元输出层。除非另有说明，否则隐藏层各有300和100个单位。

MNIST数据集由60,000个训练样例和10,000个测试示例组成。我们从训练集中随机**抽取了5,000个示例验证集**，并将**剩余的55,000个训练样例用作本文其余部分的训练集（包括第2节）**。本附录中的超参数选择实验使用验证集进行评估，以确定早期停止的迭代和早期停止的准确性;本文正文中的网络（利用这些超参数）在测试集上评估其准确性。训练集以60个例子的小批量呈现给网络;在每个时代，整个训练集都是混乱。

除非另有说明，否则每个图中的每一条线包括来自三个独立实验的数据。线本身跟踪实验的平均性能，误差条表示任何一个实验的最小和最大性能。

在本附录中，我们**迭代地执行彩票实验**，每次迭代的**修剪率为20％（输出层为10％）**;我们在本附录后面选择这种修剪率是合理的。网络的每一层都是独立修剪的。**在彩票实验的每次迭代中，无论何时发生早期停止，网络都经过50,000次训练迭代的训练**;换句话说，在训练过程中不考虑验证或测试数据，并且通过检查验证性能来追溯确定早期停止时间。我们每100次迭代评估验证和测试性能。

对于论文的主体，我们选择使用**Adam优化器**（Kingma＆Ba，2014）和**Gaussian Glorot初始化**（Glorot＆Bengio，2010）。虽然我们可以在其他超参数的彩票实验中获得更令人印象深刻的结果，但我们希望这些选择尽可能通用，以尽量减少我们的主要结果取决于手选超参数的程度。在本附录中，我们选择了在本文正文中使用的Adam的学习率。

此外，我们还考虑了其他各种超参数，包括其他优化算法（有和没有动量的SGD），初始化策略（具有各种标准偏差的高斯分布），网络大小（更大和更小的隐藏层）和修剪策略（更快）和较慢的修剪率）。在每个实验中，我们改变所选择的超参数，同时保持所有其他超参数的默认值（具有所选学习速率的Adam，Gaussian Glorot初始化，具有300和100单位的隐藏层）。本附录中提供的数据是通过Lenet架构的培训变体收集的3,000多次。

## G.2 学习率

在本小节中，我们对Lenet架构进行了彩票实验，并对Adam，SGD和SGD进行了优化，并具有各种学习率的动力。

在这里，我们选择在本文主体中用于Adam的学习率。我们选择学习率的标准如下：

1. 在未剪枝的网络上，它应该最小化实现早期停止所需的训练迭代，并最大化该迭代的验证准确性。也就是说，即使我们没有运行彩票实验，它也应该是用于优化未剪枝网络的合理超参数。
2. 在运行迭代彩票实验时，应该可以使用尽可能少的参数来匹配原始网络的早期停止迭代和准确性。
3. 在满足（1）和（2）的那些选项中，它应该在保守（慢）方面，以便更有可能在各种条件下通过各种超参数有效地优化经过大量修剪的网络。

图26显示了在执行迭代彩票实验的迭代中的早期停止迭代和验证准确性，其中Lenet架构以各种学习速率与Adam优化。根据图26右侧的图表，在0.0002和0.002之间的几个学习速率在原始网络上实现了类似的验证准确度水平，并且在网络被修剪时将该性能保持在相似的水平。在这些学习率中，**0.0012和0.002**产生最快的早停时间并将其保持在最小的网络规模。我们选择**0.0012**是因为它在未净化的网络上具有更高的验证精度并且考虑到上述标准（3）。

我们注意到，在所有这些学习率中，彩票模式（其中学习变得更快并且验证准确性随着迭代修剪而增加）仍然存在。即使对于那些在50,000次迭代（2.5e-05和0.0064）内不满足早期停止标准的学习率，仍然显示出修剪精度的提高。

## G.3 其他优化算法



## G.4 ITERATIVE PRUNING RATE 剪枝比例

当在Lenet上运行迭代彩票实验时，我们以特定的速率分别修剪网络的每一层。也就是说，在训练网络之后，我们在将权重重置为其原始初始化并再次训练之前，修剪每层中k％的权重（输出层中权重的k/2％）。在本文的主体中，**我们发现迭代修剪比单次修剪获得更少的中奖票，表明修剪过多的网络会立刻降低性能。在这里，我们探索k的不同值。**

图29显示了每次修剪迭代修剪的网络数量对早期停止时间和验证准确性的影响。在最低修剪率（0.1和0.2）和更高的修剪率（0.4及以上）之间的早期停止学习速度和验证准确性方面存在切实差异。最低的修剪率可以达到更高的验证准确度，并将验证精度保持在较小的网络规模;它们还可以将较早的停止时间保持在较小的网络规模。对于本文的主体和本附录中的实验，**我们使用0.2的修剪率**，其保持0.1的准确性和学习速度的大部分，同时减少了获得较小网络规模所需的训练迭代次数。

在所有Lenet实验中，我们以**网络其余部分的一半速率修剪输出层**。由于输出层非常小（整个Lenet架构中的266,000个中的1,000个权重），我们发现修剪它比其他层更早到达收益递减点。

## G.5 初始化分布

到目前为止，我们只考虑了网络的Gaussian Glorot（Glorot＆Bengio，2010）初始化方案。图30执行彩票实验，同时从具有各种标准偏差的高斯分布初始化Lenet架构。亚当以之前选择的学习率对网络进行了优化。彩票模式继续出现在所有标准偏差中。当从**具有标准偏差0.1的高斯分布初始化时**，Lenet架构保持高验证准确度和较低的早停时间，最长，与Glorot初始化网络的性能大致匹配。

## G.6 网络尺寸

在本节中，我们考虑了Lenet架构，在第一个隐藏层中有300个单元，在第二个隐藏层中有100个单元。图31显示了Lenet架构迭代的早期停止迭代和验证准确性以及其他几个层大小。我们测试的所有网络都保持第一个隐藏层中单元与第二个隐藏层中单元之间的比例为3：1。

彩票假设自然会邀请一系列与网络规模相关的问题。一般来说，这些问题往往采取以下形式：根据彩票假设，做更大的网络，包含更多的子网，找到“更好”的中奖票？根据这个问题的一般性，有几个不同的答案。

**如果我们按照其实现的准确度来评估中奖彩票，那么较大的网络会获得更好的中奖彩票**。图31中的右图显示，对于任何特定数量的权重（即，x轴上的任何特定点），从最初较大的网络导出的获胜票证达到更高的准确度。换句话说，就准确性而言，线路按照网络尺寸的递增顺序从下到上大致排列。有可能的是，由于较大的网络具有更多的子网，因此梯度下降找到了更好的中奖票。或者，最初较大的网络即使在修剪到与较小网络相同数量的权重时也具有更多单元，这意味着它们能够包含最初较小网络无法表达的稀疏子网配置。

**如果我们在获得早期停止所需的时间内评估中奖票，那么较大的网络就没那么大了**。图31中的左图表明，通常，早期停止迭代在已经被修剪到相同数量的权重的不同初始大小的网络之间不会有很大变化。在非常接近的检查中，从最初较大的网络获得的获胜门票往往比从最初较小的网络获得的门票略微学习，但这些差异很小

**如果我们根据其获得与原始网络相同精度的大小来评估中奖票，则大型网络没有优势**。无论初始网络规模如何，图31中的右图显示，当获奖票据被修剪到大约9,000到15,000个权重之间时，它们会恢复到原始网络的准确性。

# H 用于卷积网络的超参数探索

Conv-2，Conv-4和Conv-6架构是针对CIFAR10（Krizhevsky＆Hinton，2009）**数据集缩小的VGG**（Simonyan＆Zisserman，2014）网络架构的变体。与VGG一样，**网络由一系列模块组成。每个模块都有两层3x3卷积滤波器，后面跟一个带有步幅2的maxpool层**。在所有模块之后是两个大小为256的全连接层，接着是一个大小为10的输出层;在VGG中，全连接层的大小为4096，输出层的大小为1000.与VGG一样，第一个模块每层有64个卷积，第二个模块有128个，第三个模块有256个，等等.Conv-2 ，Conv-4和Conv-6架构分别具有1,2和3个模块。

**CIFAR10数据集包含50,000个32x32颜色（三通道）训练示例和10,000个测试示例。我们从训练集中随机抽取了5,000个示例验证集，并使用剩余的45,000个训练样例作为本文其余部分的训练集。**在整个**本附录中的超参数选择实验在验证集上进行评估**，并且在测试集上评估本文主体中的实例（使用这些超参数）。训练集以60个例子的小批量呈现给网络;在每个时代，整个训练集都是混乱。

Conv-2，Conv-4和Conv-6网络使用Gaussian Glorot初始化进行初始化（Glorot＆Bengio，2010），并针对图2中指定的迭代次数进行训练。选择的训练迭代次数如此之多-pruned网络仍然可以在提供的时间内进行训练。在辍学实验中，训练迭代次数增加三倍，为退出正则化网络提供足够的时间进行训练。我们使用Adam优化这些网络，并在本附录中选择每个网络的学习速率。



## H.4 迭代剪枝比例

对于卷积网络架构，我们为卷积和完全连接的层选择不同的修剪速率。在Conv-2和Conv-4架构中，**卷积参数占模型中参数总数的相对较小的一部分。**通过更慢地修剪卷积，我们可能能够在保持性能的同时进一步修剪模型。换句话说，我们假设，如果所有层都被均匀地修剪，卷积层将成为一个瓶颈，使得更难以找到仍然能够学习的较低参数计数模型。对于Conv-6，反之亦然：由于其近三分之二的参数位于卷积层中，修剪完全连接的层可能成为瓶颈。

我们在本节中选择超参数的标准是找到修剪率的组合，允许网络达到尽可能低的参数计数，同时保持验证准确度等于或高于原始准确度和早期停止时间等于或低于原始网络。

图35显示了对Conv-2（顶部），Conv-4（中部）和Conv-6（底部）执行迭代彩票实验的结果，其具有不同的修剪率组合。

根据我们的标准，我们选择Conv-2的迭代卷积修剪率为10％，Conv-4为10％，Conv-6为15％。对于每个网络，10％到20％之间的任何比率似乎都是合理的。在所有卷积修剪率中，彩票模式继续出现。

## H.5 学习率(dropout)

为了训练Conv-2，Conv-4和Conv-6体系结构，我们重复了H.2节中的练习，以选择合适的学习速率。图32显示了对Conv-2（上图），Conv-4（中图）和Conv-6（下图）执行迭代彩票实验的结果，其中有丢失和Adam在各种学习速率下。受到辍学训练的网络需要更长的时间来学习，因此我们训练每个架构的迭代次数是实验的三倍而没有丢失：Conv-2的60,000次迭代，Conv-4的75,000次迭代以及Conv-6的90,000次迭代。我们以第H.4节确定的速率迭代地修剪这些网络。

在我们测试的所有学习率下，彩票模式通常都是准确的，随着网络的修剪而改进。然而，并非所有学习率都表明早停时间的减少。相反，Conv-2的学习率没有显示出其他彩票实验中所见的早停时间的明显改善。同样，Conv-4和Conv-6的学习速度更快，保持原始的早期停止时间，直到修剪到大约40％，此时早停时间稳步增加



# 附录I HYPERPARAMETER EXPLORATION FOR VGG-19 AND RESNET-18 ON CIFAR10

CIFAR10上的VGG-19和RESNET-18的超参数探索

本附录附有第4节中的VGG-19和Resnet-18实验。它详细介绍了我们用于这些网络的修剪方案，培训制度和超参数

## I.1 全局剪枝

在我们使用Lenet和Conv-2/4/6架构的实验中，我们分别修剪每层中的一小部分参数（逐层修剪）。在我们使用VGG-19和Resnet-18的实验中，我们改为全球修剪;也就是说，我们共同修剪卷积层中的所有权重，而不考虑任何权重来源的特定层

图38（VGG-19）和39（Resnet-18）比较了**第4节中超参数的全局修剪（实线）和分层修剪（虚线）所获得的中奖彩票**。培训VGG-19时学习率0.1和预热迭代10,000，当Pm≥6.9％进行分层修剪而Pm≥1.5％进行全局修剪时，我们会获得中奖票。对于其他超参数，对于逐层修剪而言，与全局修剪相比，精度同样会下降。全球修剪也比Resnet-18的分层修剪获得更少的中奖票，但差异不如VGG-19差。

在第4节中，我们讨论了在更深层网络上进行全局修剪的有效性的基本原理。总之，这些深层网络中的层具有非常不同的参数数量（对于VGG-19尤为严重）;如果我们逐层修剪，我们推测参数较少的层会成为我们查找较小中奖票的能力的瓶颈。

## 改进

先剪枝，再扩展